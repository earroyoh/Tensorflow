{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Course 1 - Part 8 - Lesson 4 - Notebook.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/earroyoh/Tensorflow/blob/master/Horses-vs-Humans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RXZT2UsyIVe_",
        "outputId": "34a5df66-2899-43bc-a371-c584d948e285",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n",
        "    -O /tmp/horse-or-human.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-14 08:01:26--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 2607:f8b0:400c:c06::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149574867 (143M) [application/zip]\n",
            "Saving to: ‘/tmp/horse-or-human.zip’\n",
            "\n",
            "/tmp/horse-or-human 100%[===================>] 142.65M   116MB/s    in 1.2s    \n",
            "\n",
            "2019-05-14 08:01:27 (116 MB/s) - ‘/tmp/horse-or-human.zip’ saved [149574867/149574867]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mLij6qde6Ox",
        "colab_type": "code",
        "outputId": "81ce8a9d-2d03-4ceb-dbeb-0ac96e94c5d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip \\\n",
        "    -O /tmp/validation-horse-or-human.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-14 08:01:42--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 2607:f8b0:400c:c06::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11480187 (11M) [application/zip]\n",
            "Saving to: ‘/tmp/validation-horse-or-human.zip’\n",
            "\n",
            "\r          /tmp/vali   0%[                    ]       0  --.-KB/s               \r/tmp/validation-hor 100%[===================>]  10.95M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-05-14 08:01:43 (86.3 MB/s) - ‘/tmp/validation-horse-or-human.zip’ saved [11480187/11480187]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9brUxyTpYZHy",
        "colab_type": "text"
      },
      "source": [
        "The following python code will use the OS library to use Operating System libraries, giving you access to the file system, and the zipfile library allowing you to unzip the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PLy3pthUS0D2",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/horse-or-human')\n",
        "local_zip = '/tmp/validation-horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/validation-horse-or-human')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o-qUPyfO7Qr8"
      },
      "source": [
        "The contents of the .zip are extracted to the base directory `/tmp/horse-or-human`, which in turn each contain `horses` and `humans` subdirectories.\n",
        "\n",
        "In short: The training set is the data that is used to tell the neural network model that 'this is what a horse looks like', 'this is what a human looks like' etc. \n",
        "\n",
        "One thing to pay attention to in this sample: We do not explicitly label the images as horses or humans. If you remember with the handwriting example earlier, we had labelled 'this is a 1', 'this is a 7' etc.  Later you'll see something called an ImageGenerator being used -- and this is coded to read images from subdirectories, and automatically label them from the name of that subdirectory. So, for example, you will have a 'training' directory containing a 'horses' directory and a 'humans' one. ImageGenerator will label the images appropriately for you, reducing a coding step. \n",
        "\n",
        "Let's define each of these directories:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NR_M9nWN-K8B",
        "colab": {}
      },
      "source": [
        "# Directory with our training horse pictures\n",
        "train_horse_dir = os.path.join('/tmp/horse-or-human/horses')\n",
        "\n",
        "# Directory with our training human pictures\n",
        "train_human_dir = os.path.join('/tmp/horse-or-human/humans')\n",
        "\n",
        "# Directory with our training horse pictures\n",
        "validation_horse_dir = os.path.join('/tmp/validation-horse-or-human/validation-horses')\n",
        "\n",
        "# Directory with our training human pictures\n",
        "validation_human_dir = os.path.join('/tmp/validation-horse-or-human/validation-humans')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5oqBkNBJmtUv"
      },
      "source": [
        "## Building a Small Model from Scratch\n",
        "\n",
        "But before we continue, let's start defining the model:\n",
        "\n",
        "Step 1 will be to import tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvfZg3LQbD-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BnhYCP4tdqjC"
      },
      "source": [
        "We then add convolutional layers as in the previous example, and flatten the final result to feed into the densely connected layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gokG5HKpdtzm",
        "colab_type": "text"
      },
      "source": [
        "Finally we add the densely connected layers. \n",
        "\n",
        "Note that because we are facing a two-class classification problem, i.e. a *binary classification problem*, we will end our network with a [*sigmoid* activation](https://wikipedia.org/wiki/Sigmoid_function), so that the output of our network will be a single scalar between 0 and 1, encoding the probability that the current image is class 1 (as opposed to class 0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PixZ2s5QbYQ3",
        "colab_type": "code",
        "outputId": "69a7dab3-63f6-432f-af4f-18f2684748b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "  \n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "pre_trained_model = InceptionV3(input_shape = (150, 150, 3), \n",
        "                                include_top = False, \n",
        "                                weights = None)\n",
        "\n",
        "pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "for layer in pre_trained_model.layers:\n",
        "  layer.trainable = False\n",
        "  \n",
        "# pre_trained_model.summary()\n",
        "\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "print('last layer output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output\n",
        "\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(last_output)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "# Add a dropout rate of 0.3\n",
        "x = layers.Dropout(0.3)(x)                  \n",
        "# Add a final sigmoid layer for classification\n",
        "x = layers.Dense  (1, activation='sigmoid')(x)           \n",
        "\n",
        "model = Model( pre_trained_model.input, x) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-14 08:43:10--  https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.141.128, 2607:f8b0:400c:c06::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.141.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87910968 (84M) [application/x-hdf]\n",
            "Saving to: ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’\n",
            "\n",
            "\r          /tmp/ince   0%[                    ]       0  --.-KB/s               \r         /tmp/incep  29%[====>               ]  24.81M   124MB/s               \r        /tmp/incept  76%[==============>     ]  64.20M   160MB/s               \r/tmp/inception_v3_w 100%[===================>]  83.84M   173MB/s    in 0.5s    \n",
            "\n",
            "2019-05-14 08:43:11 (173 MB/s) - ‘/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5’ saved [87910968/87910968]\n",
            "\n",
            "last layer output shape:  (None, 7, 7, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s9EaFDP5srBa"
      },
      "source": [
        "The model.summary() method call prints a summary of the NN "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7ZKj8392nbgP",
        "outputId": "c4120ae7-17f6-4ef1-ffc2-34d59827e2ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9127
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 150, 150, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 74, 74, 32)   864         input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_94 (Batc (None, 74, 74, 32)   96          conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 74, 74, 32)   0           batch_normalization_v1_94[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_95 (Conv2D)              (None, 72, 72, 32)   9216        activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_95 (Batc (None, 72, 72, 32)   96          conv2d_95[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 72, 72, 32)   0           batch_normalization_v1_95[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_96 (Conv2D)              (None, 72, 72, 64)   18432       activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_96 (Batc (None, 72, 72, 64)   192         conv2d_96[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 72, 72, 64)   0           batch_normalization_v1_96[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 35, 35, 64)   0           activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_97 (Conv2D)              (None, 35, 35, 80)   5120        max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_97 (Batc (None, 35, 35, 80)   240         conv2d_97[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 35, 35, 80)   0           batch_normalization_v1_97[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_98 (Conv2D)              (None, 33, 33, 192)  138240      activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_98 (Batc (None, 33, 33, 192)  576         conv2d_98[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 33, 33, 192)  0           batch_normalization_v1_98[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 16, 16, 192)  0           activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_102 (Conv2D)             (None, 16, 16, 64)   12288       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_102 (Bat (None, 16, 16, 64)   192         conv2d_102[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_102 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_102[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_100 (Conv2D)             (None, 16, 16, 48)   9216        max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_103 (Conv2D)             (None, 16, 16, 96)   55296       activation_102[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_100 (Bat (None, 16, 16, 48)   144         conv2d_100[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_103 (Bat (None, 16, 16, 96)   288         conv2d_103[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_100 (Activation)     (None, 16, 16, 48)   0           batch_normalization_v1_100[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_103 (Activation)     (None, 16, 16, 96)   0           batch_normalization_v1_103[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_9 (AveragePoo (None, 16, 16, 192)  0           max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_99 (Conv2D)              (None, 16, 16, 64)   12288       max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_101 (Conv2D)             (None, 16, 16, 64)   76800       activation_100[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_104 (Conv2D)             (None, 16, 16, 96)   82944       activation_103[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_105 (Conv2D)             (None, 16, 16, 32)   6144        average_pooling2d_9[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_99 (Batc (None, 16, 16, 64)   192         conv2d_99[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_101 (Bat (None, 16, 16, 64)   192         conv2d_101[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_104 (Bat (None, 16, 16, 96)   288         conv2d_104[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_105 (Bat (None, 16, 16, 32)   96          conv2d_105[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, 16, 16, 64)   0           batch_normalization_v1_99[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "activation_101 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_101[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_104 (Activation)     (None, 16, 16, 96)   0           batch_normalization_v1_104[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_105 (Activation)     (None, 16, 16, 32)   0           batch_normalization_v1_105[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 16, 16, 256)  0           activation_99[0][0]              \n",
            "                                                                 activation_101[0][0]             \n",
            "                                                                 activation_104[0][0]             \n",
            "                                                                 activation_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_109 (Conv2D)             (None, 16, 16, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_109 (Bat (None, 16, 16, 64)   192         conv2d_109[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_109 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_109[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_107 (Conv2D)             (None, 16, 16, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_110 (Conv2D)             (None, 16, 16, 96)   55296       activation_109[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_107 (Bat (None, 16, 16, 48)   144         conv2d_107[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_110 (Bat (None, 16, 16, 96)   288         conv2d_110[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_107 (Activation)     (None, 16, 16, 48)   0           batch_normalization_v1_107[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_110 (Activation)     (None, 16, 16, 96)   0           batch_normalization_v1_110[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_10 (AveragePo (None, 16, 16, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_106 (Conv2D)             (None, 16, 16, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_108 (Conv2D)             (None, 16, 16, 64)   76800       activation_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_111 (Conv2D)             (None, 16, 16, 96)   82944       activation_110[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_112 (Conv2D)             (None, 16, 16, 64)   16384       average_pooling2d_10[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_106 (Bat (None, 16, 16, 64)   192         conv2d_106[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_108 (Bat (None, 16, 16, 64)   192         conv2d_108[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_111 (Bat (None, 16, 16, 96)   288         conv2d_111[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_112 (Bat (None, 16, 16, 64)   192         conv2d_112[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_106 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_106[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_108 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_108[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_111 (Activation)     (None, 16, 16, 96)   0           batch_normalization_v1_111[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_112 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_112[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 16, 16, 288)  0           activation_106[0][0]             \n",
            "                                                                 activation_108[0][0]             \n",
            "                                                                 activation_111[0][0]             \n",
            "                                                                 activation_112[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_116 (Conv2D)             (None, 16, 16, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_116 (Bat (None, 16, 16, 64)   192         conv2d_116[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_116 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_116[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_114 (Conv2D)             (None, 16, 16, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_117 (Conv2D)             (None, 16, 16, 96)   55296       activation_116[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_114 (Bat (None, 16, 16, 48)   144         conv2d_114[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_117 (Bat (None, 16, 16, 96)   288         conv2d_117[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_114 (Activation)     (None, 16, 16, 48)   0           batch_normalization_v1_114[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_117 (Activation)     (None, 16, 16, 96)   0           batch_normalization_v1_117[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_11 (AveragePo (None, 16, 16, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_113 (Conv2D)             (None, 16, 16, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_115 (Conv2D)             (None, 16, 16, 64)   76800       activation_114[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_118 (Conv2D)             (None, 16, 16, 96)   82944       activation_117[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_119 (Conv2D)             (None, 16, 16, 64)   18432       average_pooling2d_11[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_113 (Bat (None, 16, 16, 64)   192         conv2d_113[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_115 (Bat (None, 16, 16, 64)   192         conv2d_115[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_118 (Bat (None, 16, 16, 96)   288         conv2d_118[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_119 (Bat (None, 16, 16, 64)   192         conv2d_119[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_113 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_113[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_115 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_115[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_118 (Activation)     (None, 16, 16, 96)   0           batch_normalization_v1_118[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_119 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_119[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 16, 16, 288)  0           activation_113[0][0]             \n",
            "                                                                 activation_115[0][0]             \n",
            "                                                                 activation_118[0][0]             \n",
            "                                                                 activation_119[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_121 (Conv2D)             (None, 16, 16, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_121 (Bat (None, 16, 16, 64)   192         conv2d_121[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_121 (Activation)     (None, 16, 16, 64)   0           batch_normalization_v1_121[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_122 (Conv2D)             (None, 16, 16, 96)   55296       activation_121[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_122 (Bat (None, 16, 16, 96)   288         conv2d_122[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_122 (Activation)     (None, 16, 16, 96)   0           batch_normalization_v1_122[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_120 (Conv2D)             (None, 7, 7, 384)    995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_123 (Conv2D)             (None, 7, 7, 96)     82944       activation_122[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_120 (Bat (None, 7, 7, 384)    1152        conv2d_120[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_123 (Bat (None, 7, 7, 96)     288         conv2d_123[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_120 (Activation)     (None, 7, 7, 384)    0           batch_normalization_v1_120[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_123 (Activation)     (None, 7, 7, 96)     0           batch_normalization_v1_123[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2D)  (None, 7, 7, 288)    0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 7, 7, 768)    0           activation_120[0][0]             \n",
            "                                                                 activation_123[0][0]             \n",
            "                                                                 max_pooling2d_6[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_128 (Conv2D)             (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_128 (Bat (None, 7, 7, 128)    384         conv2d_128[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 7, 7, 128)    0           batch_normalization_v1_128[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_129 (Conv2D)             (None, 7, 7, 128)    114688      activation_128[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_129 (Bat (None, 7, 7, 128)    384         conv2d_129[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 7, 7, 128)    0           batch_normalization_v1_129[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_125 (Conv2D)             (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_130 (Conv2D)             (None, 7, 7, 128)    114688      activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_125 (Bat (None, 7, 7, 128)    384         conv2d_125[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_130 (Bat (None, 7, 7, 128)    384         conv2d_130[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_125 (Activation)     (None, 7, 7, 128)    0           batch_normalization_v1_125[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_130 (Activation)     (None, 7, 7, 128)    0           batch_normalization_v1_130[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_126 (Conv2D)             (None, 7, 7, 128)    114688      activation_125[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_131 (Conv2D)             (None, 7, 7, 128)    114688      activation_130[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_126 (Bat (None, 7, 7, 128)    384         conv2d_126[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_131 (Bat (None, 7, 7, 128)    384         conv2d_131[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_126 (Activation)     (None, 7, 7, 128)    0           batch_normalization_v1_126[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_131 (Activation)     (None, 7, 7, 128)    0           batch_normalization_v1_131[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_12 (AveragePo (None, 7, 7, 768)    0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_124 (Conv2D)             (None, 7, 7, 192)    147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_127 (Conv2D)             (None, 7, 7, 192)    172032      activation_126[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_132 (Conv2D)             (None, 7, 7, 192)    172032      activation_131[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_133 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_12[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_124 (Bat (None, 7, 7, 192)    576         conv2d_124[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_127 (Bat (None, 7, 7, 192)    576         conv2d_127[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_132 (Bat (None, 7, 7, 192)    576         conv2d_132[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_133 (Bat (None, 7, 7, 192)    576         conv2d_133[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_124 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_124[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_127[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_132 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_132[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_133 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_133[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 7, 7, 768)    0           activation_124[0][0]             \n",
            "                                                                 activation_127[0][0]             \n",
            "                                                                 activation_132[0][0]             \n",
            "                                                                 activation_133[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_138 (Conv2D)             (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_138 (Bat (None, 7, 7, 160)    480         conv2d_138[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_138 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_138[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_139 (Conv2D)             (None, 7, 7, 160)    179200      activation_138[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_139 (Bat (None, 7, 7, 160)    480         conv2d_139[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_139 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_139[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_135 (Conv2D)             (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_140 (Conv2D)             (None, 7, 7, 160)    179200      activation_139[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_135 (Bat (None, 7, 7, 160)    480         conv2d_135[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_140 (Bat (None, 7, 7, 160)    480         conv2d_140[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_135 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_135[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_140 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_140[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_136 (Conv2D)             (None, 7, 7, 160)    179200      activation_135[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_141 (Conv2D)             (None, 7, 7, 160)    179200      activation_140[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_136 (Bat (None, 7, 7, 160)    480         conv2d_136[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_141 (Bat (None, 7, 7, 160)    480         conv2d_141[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_136 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_136[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_141 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_141[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_13 (AveragePo (None, 7, 7, 768)    0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_134 (Conv2D)             (None, 7, 7, 192)    147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_137 (Conv2D)             (None, 7, 7, 192)    215040      activation_136[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_142 (Conv2D)             (None, 7, 7, 192)    215040      activation_141[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_143 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_13[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_134 (Bat (None, 7, 7, 192)    576         conv2d_134[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_137 (Bat (None, 7, 7, 192)    576         conv2d_137[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_142 (Bat (None, 7, 7, 192)    576         conv2d_142[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_143 (Bat (None, 7, 7, 192)    576         conv2d_143[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_134 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_134[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_137 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_137[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_142 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_142[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_143 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_143[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 7, 7, 768)    0           activation_134[0][0]             \n",
            "                                                                 activation_137[0][0]             \n",
            "                                                                 activation_142[0][0]             \n",
            "                                                                 activation_143[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_148 (Conv2D)             (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_148 (Bat (None, 7, 7, 160)    480         conv2d_148[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_148 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_148[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_149 (Conv2D)             (None, 7, 7, 160)    179200      activation_148[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_149 (Bat (None, 7, 7, 160)    480         conv2d_149[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_149 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_149[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_145 (Conv2D)             (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_150 (Conv2D)             (None, 7, 7, 160)    179200      activation_149[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_145 (Bat (None, 7, 7, 160)    480         conv2d_145[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_150 (Bat (None, 7, 7, 160)    480         conv2d_150[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_145 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_145[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_150 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_150[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_146 (Conv2D)             (None, 7, 7, 160)    179200      activation_145[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_151 (Conv2D)             (None, 7, 7, 160)    179200      activation_150[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_146 (Bat (None, 7, 7, 160)    480         conv2d_146[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_151 (Bat (None, 7, 7, 160)    480         conv2d_151[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_146 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_146[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_151 (Activation)     (None, 7, 7, 160)    0           batch_normalization_v1_151[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_14 (AveragePo (None, 7, 7, 768)    0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_144 (Conv2D)             (None, 7, 7, 192)    147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_147 (Conv2D)             (None, 7, 7, 192)    215040      activation_146[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_152 (Conv2D)             (None, 7, 7, 192)    215040      activation_151[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_153 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_14[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_144 (Bat (None, 7, 7, 192)    576         conv2d_144[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_147 (Bat (None, 7, 7, 192)    576         conv2d_147[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_152 (Bat (None, 7, 7, 192)    576         conv2d_152[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_153 (Bat (None, 7, 7, 192)    576         conv2d_153[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_144 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_144[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_147 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_147[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_152 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_152[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_153 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_153[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 7, 7, 768)    0           activation_144[0][0]             \n",
            "                                                                 activation_147[0][0]             \n",
            "                                                                 activation_152[0][0]             \n",
            "                                                                 activation_153[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_158 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_158 (Bat (None, 7, 7, 192)    576         conv2d_158[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_158 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_158[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_159 (Conv2D)             (None, 7, 7, 192)    258048      activation_158[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_159 (Bat (None, 7, 7, 192)    576         conv2d_159[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_159 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_159[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_155 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_160 (Conv2D)             (None, 7, 7, 192)    258048      activation_159[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_155 (Bat (None, 7, 7, 192)    576         conv2d_155[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_160 (Bat (None, 7, 7, 192)    576         conv2d_160[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_155 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_155[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_160 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_160[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_156 (Conv2D)             (None, 7, 7, 192)    258048      activation_155[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_161 (Conv2D)             (None, 7, 7, 192)    258048      activation_160[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_156 (Bat (None, 7, 7, 192)    576         conv2d_156[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_161 (Bat (None, 7, 7, 192)    576         conv2d_161[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_156 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_156[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_161 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_161[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_15 (AveragePo (None, 7, 7, 768)    0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_154 (Conv2D)             (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_157 (Conv2D)             (None, 7, 7, 192)    258048      activation_156[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_162 (Conv2D)             (None, 7, 7, 192)    258048      activation_161[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_163 (Conv2D)             (None, 7, 7, 192)    147456      average_pooling2d_15[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_154 (Bat (None, 7, 7, 192)    576         conv2d_154[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_157 (Bat (None, 7, 7, 192)    576         conv2d_157[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_162 (Bat (None, 7, 7, 192)    576         conv2d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_v1_163 (Bat (None, 7, 7, 192)    576         conv2d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_154 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_154[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_157 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_157[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_162 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_162[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "activation_163 (Activation)     (None, 7, 7, 192)    0           batch_normalization_v1_163[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 7, 7, 768)    0           activation_154[0][0]             \n",
            "                                                                 activation_157[0][0]             \n",
            "                                                                 activation_162[0][0]             \n",
            "                                                                 activation_163[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 37632)        0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 1024)         38536192    flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1024)         0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 1)            1025        dropout_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 47,512,481\n",
            "Trainable params: 38,537,217\n",
            "Non-trainable params: 8,975,264\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DmtkTn06pKxF"
      },
      "source": [
        "The \"output shape\" column shows how the size of your feature map evolves in each successive layer. The convolution layers reduce the size of the feature maps by a bit due to padding, and each pooling layer halves the dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PEkKSpZlvJXA"
      },
      "source": [
        "Next, we'll configure the specifications for model training. We will train our model with the `binary_crossentropy` loss, because it's a binary classification problem and our final activation is a sigmoid. (For a refresher on loss metrics, see the [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/descending-into-ml/video-lecture).) We will use the `rmsprop` optimizer with a learning rate of `0.001`. During training, we will want to monitor classification accuracy.\n",
        "\n",
        "**NOTE**: In this case, using the [RMSprop optimization algorithm](https://wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp) is preferable to [stochastic gradient descent](https://developers.google.com/machine-learning/glossary/#SGD) (SGD), because RMSprop automates learning-rate tuning for us. (Other optimizers, such as [Adam](https://wikipedia.org/wiki/Stochastic_gradient_descent#Adam) and [Adagrad](https://developers.google.com/machine-learning/glossary/#AdaGrad), also automatically adapt the learning rate during training, and would work equally well here.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8DHWhFP_uhq3",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Sn9m9D3UimHM"
      },
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "Let's set up data generators that will read pictures in our source folders, convert them to `float32` tensors, and feed them (with their labels) to our network. We'll have one generator for the training images and one for the validation images. Our generators will yield batches of images of size 300x300 and their labels (binary).\n",
        "\n",
        "As you may already know, data that goes into neural networks should usually be normalized in some way to make it more amenable to processing by the network. (It is uncommon to feed raw pixels into a convnet.) In our case, we will preprocess our images by normalizing the pixel values to be in the `[0, 1]` range (originally all values are in the `[0, 255]` range).\n",
        "\n",
        "In Keras this can be done via the `keras.preprocessing.image.ImageDataGenerator` class using the `rescale` parameter. This `ImageDataGenerator` class allows you to instantiate generators of augmented image batches (and their labels) via `.flow(data, labels)` or `.flow_from_directory(directory)`. These generators can then be used with the Keras model methods that accept data generators as inputs: `fit_generator`, `evaluate_generator`, and `predict_generator`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ClebU9NJg99G",
        "outputId": "47e7c178-210c-48a6-d08d-a0771b7e7133",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/tmp/horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=128,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        '/tmp/validation-horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=32,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mu3Jdwkjwax4"
      },
      "source": [
        "### Training\n",
        "Let's train for 15 epochs -- this may take a few minutes to run.\n",
        "\n",
        "Do note the values per epoch.\n",
        "\n",
        "The Loss and Accuracy are a great indication of progress of training. It's making a guess as to the classification of the training data, and then measuring it against the known label, calculating the result. Accuracy is the portion of correct guesses. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fb1_lgobv81m",
        "outputId": "a3630f00-261e-4881-d8ed-a7e175f43ff9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2221
        }
      },
      "source": [
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=8,  \n",
        "      epochs=15,\n",
        "      verbose=1,\n",
        "      validation_data = validation_generator,\n",
        "      validation_steps=8)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "8/8 [==============================] - 4s 545ms/step - loss: 0.1359 - acc: 0.9805\n",
            "9/9 [==============================] - 8s 922ms/step - loss: 1.0952e-07 - acc: 1.0000 - val_loss: 0.1359 - val_acc: 0.9805\n",
            "Epoch 2/40\n",
            "8/8 [==============================] - 4s 448ms/step - loss: 0.2913 - acc: 0.9766\n",
            "9/9 [==============================] - 8s 935ms/step - loss: 1.1077e-07 - acc: 1.0000 - val_loss: 0.2913 - val_acc: 0.9766\n",
            "Epoch 3/40\n",
            "8/8 [==============================] - 4s 450ms/step - loss: 0.2050 - acc: 0.9805\n",
            "9/9 [==============================] - 8s 931ms/step - loss: 1.1015e-07 - acc: 1.0000 - val_loss: 0.2050 - val_acc: 0.9805\n",
            "Epoch 4/40\n",
            "8/8 [==============================] - 4s 453ms/step - loss: 0.1720 - acc: 0.9805\n",
            "9/9 [==============================] - 8s 935ms/step - loss: 1.1015e-07 - acc: 1.0000 - val_loss: 0.1720 - val_acc: 0.9805\n",
            "Epoch 5/40\n",
            "8/8 [==============================] - 4s 451ms/step - loss: 0.1674 - acc: 0.9883\n",
            "9/9 [==============================] - 8s 926ms/step - loss: 1.0952e-07 - acc: 1.0000 - val_loss: 0.1674 - val_acc: 0.9883\n",
            "Epoch 6/40\n",
            "8/8 [==============================] - 4s 454ms/step - loss: 0.0964 - acc: 0.9805\n",
            "9/9 [==============================] - 8s 928ms/step - loss: 1.1015e-07 - acc: 1.0000 - val_loss: 0.0964 - val_acc: 0.9805\n",
            "Epoch 7/40\n",
            "8/8 [==============================] - 4s 460ms/step - loss: 0.1081 - acc: 0.9805\n",
            "9/9 [==============================] - 8s 934ms/step - loss: 1.1015e-07 - acc: 1.0000 - val_loss: 0.1081 - val_acc: 0.9805\n",
            "Epoch 8/40\n",
            "8/8 [==============================] - 4s 451ms/step - loss: 0.2911 - acc: 0.9727\n",
            "9/9 [==============================] - 8s 934ms/step - loss: 1.0889e-07 - acc: 1.0000 - val_loss: 0.2911 - val_acc: 0.9727\n",
            "Epoch 9/40\n",
            "8/8 [==============================] - 4s 451ms/step - loss: 0.1366 - acc: 0.9805\n",
            "9/9 [==============================] - 8s 930ms/step - loss: 1.1015e-07 - acc: 1.0000 - val_loss: 0.1366 - val_acc: 0.9805\n",
            "Epoch 10/40\n",
            "8/8 [==============================] - 4s 456ms/step - loss: 0.4217 - acc: 0.9609\n",
            "9/9 [==============================] - 8s 925ms/step - loss: 1.0952e-07 - acc: 1.0000 - val_loss: 0.4217 - val_acc: 0.9609\n",
            "Epoch 11/40\n",
            "8/8 [==============================] - 4s 452ms/step - loss: 0.1434 - acc: 0.9883\n",
            "9/9 [==============================] - 8s 941ms/step - loss: 1.0952e-07 - acc: 1.0000 - val_loss: 0.1434 - val_acc: 0.9883\n",
            "Epoch 12/40\n",
            "8/8 [==============================] - 4s 446ms/step - loss: 0.2939 - acc: 0.9609\n",
            "9/9 [==============================] - 8s 923ms/step - loss: 1.0952e-07 - acc: 1.0000 - val_loss: 0.2939 - val_acc: 0.9609\n",
            "Epoch 13/40\n",
            "8/8 [==============================] - 4s 439ms/step - loss: 0.1060 - acc: 0.9883\n",
            "9/9 [==============================] - 8s 942ms/step - loss: 1.1015e-07 - acc: 1.0000 - val_loss: 0.1060 - val_acc: 0.9883\n",
            "Epoch 14/40\n",
            "8/8 [==============================] - 4s 450ms/step - loss: 0.3064 - acc: 0.9727\n",
            "9/9 [==============================] - 8s 917ms/step - loss: 1.1015e-07 - acc: 1.0000 - val_loss: 0.3064 - val_acc: 0.9727\n",
            "Epoch 15/40\n",
            "8/8 [==============================] - 4s 449ms/step - loss: 0.1945 - acc: 0.9844\n",
            "9/9 [==============================] - 8s 928ms/step - loss: 1.1077e-07 - acc: 1.0000 - val_loss: 0.1945 - val_acc: 0.9844\n",
            "Epoch 16/40\n",
            "8/8 [==============================] - 4s 444ms/step - loss: 0.0035 - acc: 1.0000\n",
            "9/9 [==============================] - 8s 911ms/step - loss: 2.7664e-07 - acc: 1.0000 - val_loss: 0.0035 - val_acc: 1.0000\n",
            "Epoch 17/40\n",
            "8/8 [==============================] - 4s 446ms/step - loss: 0.0630 - acc: 0.9961\n",
            "9/9 [==============================] - 8s 916ms/step - loss: 1.0952e-07 - acc: 1.0000 - val_loss: 0.0630 - val_acc: 0.9961\n",
            "Epoch 18/40\n",
            "8/8 [==============================] - 3s 435ms/step - loss: 0.0799 - acc: 0.9922\n",
            "9/9 [==============================] - 8s 913ms/step - loss: 1.1015e-07 - acc: 1.0000 - val_loss: 0.0799 - val_acc: 0.9922\n",
            "Epoch 19/40\n",
            "8/8 [==============================] - 4s 452ms/step - loss: 0.1064 - acc: 0.9883\n",
            "9/9 [==============================] - 8s 926ms/step - loss: 1.0889e-07 - acc: 1.0000 - val_loss: 0.1064 - val_acc: 0.9883\n",
            "Epoch 20/40\n",
            "8/8 [==============================] - 4s 449ms/step - loss: 0.1156 - acc: 0.9922\n",
            "9/9 [==============================] - 8s 921ms/step - loss: 1.1015e-07 - acc: 1.0000 - val_loss: 0.1156 - val_acc: 0.9922\n",
            "Epoch 21/40\n",
            "8/8 [==============================] - 4s 450ms/step - loss: 0.0867 - acc: 0.9922\n",
            "9/9 [==============================] - 8s 938ms/step - loss: 1.0952e-07 - acc: 1.0000 - val_loss: 0.0867 - val_acc: 0.9922\n",
            "Epoch 22/40\n",
            "8/8 [==============================] - 4s 452ms/step - loss: 0.0982 - acc: 0.9922\n",
            "9/9 [==============================] - 8s 927ms/step - loss: 1.1015e-07 - acc: 1.0000 - val_loss: 0.0982 - val_acc: 0.9922\n",
            "Epoch 23/40\n",
            "8/8 [==============================] - 4s 447ms/step - loss: 0.0396 - acc: 0.9961\n",
            "9/9 [==============================] - 8s 928ms/step - loss: 1.1015e-07 - acc: 1.0000 - val_loss: 0.0396 - val_acc: 0.9961\n",
            "Epoch 24/40\n",
            "8/8 [==============================] - 4s 451ms/step - loss: 0.0368 - acc: 0.9961\n",
            "9/9 [==============================] - 9s 952ms/step - loss: 1.0889e-07 - acc: 1.0000 - val_loss: 0.0368 - val_acc: 0.9961\n",
            "Epoch 25/40\n",
            "8/8 [==============================] - 4s 454ms/step - loss: 0.0756 - acc: 0.9883\n",
            "9/9 [==============================] - 8s 943ms/step - loss: 1.1015e-07 - acc: 1.0000 - val_loss: 0.0756 - val_acc: 0.9883\n",
            "Epoch 26/40\n",
            "8/8 [==============================] - 4s 451ms/step - loss: 0.0550 - acc: 0.9922\n",
            "9/9 [==============================] - 9s 957ms/step - loss: 1.1015e-07 - acc: 1.0000 - val_loss: 0.0550 - val_acc: 0.9922\n",
            "Epoch 27/40\n",
            "8/8 [==============================] - 4s 455ms/step - loss: 0.1498 - acc: 0.9844\n",
            "9/9 [==============================] - 8s 923ms/step - loss: 1.0962e-07 - acc: 1.0000 - val_loss: 0.1498 - val_acc: 0.9844\n",
            "Epoch 28/40\n",
            "8/8 [==============================] - 4s 452ms/step - loss: 0.1610 - acc: 0.9844\n",
            "9/9 [==============================] - 8s 928ms/step - loss: 1.1015e-07 - acc: 1.0000 - val_loss: 0.1610 - val_acc: 0.9844\n",
            "Epoch 29/40\n",
            "8/8 [==============================] - 4s 441ms/step - loss: 0.0592 - acc: 0.9922\n",
            "9/9 [==============================] - 8s 932ms/step - loss: 1.0952e-07 - acc: 1.0000 - val_loss: 0.0592 - val_acc: 0.9922\n",
            "Epoch 30/40\n",
            "8/8 [==============================] - 4s 451ms/step - loss: 0.1239 - acc: 0.9883\n",
            "9/9 [==============================] - 9s 945ms/step - loss: 1.1015e-07 - acc: 1.0000 - val_loss: 0.1239 - val_acc: 0.9883\n",
            "Epoch 31/40\n",
            "8/8 [==============================] - 4s 457ms/step - loss: 0.0442 - acc: 0.9961\n",
            "9/9 [==============================] - 8s 938ms/step - loss: 1.0952e-07 - acc: 1.0000 - val_loss: 0.0442 - val_acc: 0.9961\n",
            "Epoch 32/40\n",
            "8/8 [==============================] - 4s 457ms/step - loss: 0.0630 - acc: 0.9961\n",
            "9/9 [==============================] - 8s 943ms/step - loss: 1.1137e-07 - acc: 1.0000 - val_loss: 0.0630 - val_acc: 0.9961\n",
            "Epoch 33/40\n",
            "8/8 [==============================] - 4s 452ms/step - loss: 0.0741 - acc: 0.9922\n",
            "9/9 [==============================] - 8s 933ms/step - loss: 1.1015e-07 - acc: 1.0000 - val_loss: 0.0741 - val_acc: 0.9922\n",
            "Epoch 34/40\n",
            "8/8 [==============================] - 4s 447ms/step - loss: 0.0640 - acc: 0.9961\n",
            "9/9 [==============================] - 8s 912ms/step - loss: 1.0952e-07 - acc: 1.0000 - val_loss: 0.0640 - val_acc: 0.9961\n",
            "Epoch 35/40\n",
            "8/8 [==============================] - 4s 449ms/step - loss: 0.0307 - acc: 0.9961\n",
            "9/9 [==============================] - 8s 916ms/step - loss: 1.1015e-07 - acc: 1.0000 - val_loss: 0.0307 - val_acc: 0.9961\n",
            "Epoch 36/40\n",
            "8/8 [==============================] - 4s 446ms/step - loss: 0.0015 - acc: 1.0000\n",
            "9/9 [==============================] - 8s 929ms/step - loss: 1.0952e-07 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 1.0000\n",
            "Epoch 37/40\n",
            "8/8 [==============================] - 4s 453ms/step - loss: 0.0918 - acc: 0.9883\n",
            "9/9 [==============================] - 9s 947ms/step - loss: 1.1077e-07 - acc: 1.0000 - val_loss: 0.0918 - val_acc: 0.9883\n",
            "Epoch 38/40\n",
            "8/8 [==============================] - 4s 455ms/step - loss: 0.0150 - acc: 0.9961\n",
            "9/9 [==============================] - 8s 931ms/step - loss: 1.0889e-07 - acc: 1.0000 - val_loss: 0.0150 - val_acc: 0.9961\n",
            "Epoch 39/40\n",
            "8/8 [==============================] - 4s 452ms/step - loss: 0.1508 - acc: 0.9883\n",
            "9/9 [==============================] - 9s 948ms/step - loss: 1.2692e-07 - acc: 1.0000 - val_loss: 0.1508 - val_acc: 0.9883\n",
            "Epoch 40/40\n",
            "8/8 [==============================] - 4s 452ms/step - loss: 0.1040 - acc: 0.9922\n",
            "9/9 [==============================] - 8s 935ms/step - loss: 1.0952e-07 - acc: 1.0000 - val_loss: 0.1040 - val_acc: 0.9922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WF5y2l4hknb",
        "colab_type": "text"
      },
      "source": [
        "### Plot accuracy\n",
        "\n",
        "Plot training and validation accuracy to check overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1dQ3LUvhViK",
        "colab_type": "code",
        "outputId": "8746b1ca-9a9f-4559-e640-802280521841",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEICAYAAABfz4NwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6vSHzPR2ghH",
        "colab_type": "text"
      },
      "source": [
        "###Running the Model\n",
        "\n",
        "Let's now take a look at actually running a prediction using the model. This code will allow you to choose 1 or more files from your file system, it will then upload them, and run them through the model, giving an indication of whether the object is a horse or a human."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoWp43WxJDNT",
        "colab_type": "code",
        "outputId": "c4565e7f-1ebe-45b3-db41-578f7aca0e65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(150, 150))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a human\")\n",
        "  else:\n",
        "    print(fn + \" is a horse\")\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7214dc18-ad44-414e-9b40-250dca511d32\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-7214dc18-ad44-414e-9b40-250dca511d32\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-83f6644adb6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m   result = _output.eval_js(\n\u001b[1;32m     63\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m---> 64\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: google.colab._files is undefined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-8EHQyWGDvWz"
      },
      "source": [
        "### Visualizing Intermediate Representations\n",
        "\n",
        "To get a feel for what kind of features our convnet has learned, one fun thing to do is to visualize how an input gets transformed as it goes through the convnet.\n",
        "\n",
        "Let's pick a random image from the training set, and then generate a figure where each row is the output of a layer, and each image in the row is a specific filter in that output feature map. Rerun this cell to generate intermediate representations for a variety of training images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-5tES8rXFjux",
        "outputId": "68f394bf-3918-44e1-a5a4-fb6bbc9676e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "# Let's define a new Model that will take an image as input, and will output\n",
        "# intermediate representations for all layers in the previous model after\n",
        "# the first.\n",
        "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
        "#visualization_model = Model(img_input, successive_outputs)\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
        "# Let's prepare a random input image from the training set.\n",
        "train_horse_names = os.listdir(train_horse_dir)\n",
        "train_human_names = os.listdir(train_human_dir)\n",
        "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
        "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
        "img_path = random.choice(horse_img_files + human_img_files)\n",
        "\n",
        "img = load_img(img_path, target_size=(150, 150))  # this is a PIL image\n",
        "x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n",
        "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n",
        "\n",
        "# Rescale by 1/255\n",
        "x /= 255\n",
        "\n",
        "# Let's run our image through our network, thus obtaining all\n",
        "# intermediate representations for this image.\n",
        "successive_feature_maps = visualization_model.predict(x)\n",
        "\n",
        "# These are the names of the layers, so can have them as part of our plot\n",
        "layer_names = [layer.name for layer in model.layers]\n",
        "\n",
        "# Now let's display our representations\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "  if len(feature_map.shape) == 4:\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
        "    # The feature map has shape (1, size, size, n_features)\n",
        "    size = feature_map.shape[1]\n",
        "    # We will tile our images in this matrix\n",
        "    display_grid = np.zeros((size, size * n_features))\n",
        "    for i in range(n_features):\n",
        "      # Postprocess the feature to make it visually palatable\n",
        "      x = feature_map[0, :, :, i]\n",
        "      x -= x.mean()\n",
        "      x /= x.std()\n",
        "      x *= 64\n",
        "      x += 128\n",
        "      x = np.clip(x, 0, 255).astype('uint8')\n",
        "      # We'll tile each filter into this big horizontal grid\n",
        "      display_grid[:, i * size : (i + 1) * size] = x\n",
        "    # Display the grid\n",
        "    scale = 20. / n_features\n",
        "    plt.figure(figsize=(scale * n_features, scale))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-410a39ea43f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mvisualization_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuccessive_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Let's prepare a random input image from the training set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mhorse_img_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_horse_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_horse_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mhuman_img_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_human_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_human_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhorse_img_files\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhuman_img_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_horse_names' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tuqK2arJL0wo"
      },
      "source": [
        "As you can see we go from the raw pixels of the images to increasingly abstract and compact representations. The representations downstream start highlighting what the network pays attention to, and they show fewer and fewer features being \"activated\"; most are set to zero. This is called \"sparsity.\" Representation sparsity is a key feature of deep learning.\n",
        "\n",
        "\n",
        "These representations carry increasingly less information about the original pixels of the image, but increasingly refined information about the class of the image. You can think of a convnet (or a deep network in general) as an information distillation pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j4IBgYCYooGD"
      },
      "source": [
        "## Clean Up\n",
        "\n",
        "Before running the next exercise, run the following cell to terminate the kernel and free memory resources:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "651IgjLyo-Jx",
        "colab": {}
      },
      "source": [
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
